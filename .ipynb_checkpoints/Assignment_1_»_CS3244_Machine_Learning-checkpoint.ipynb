{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ORVJTRAh-Z2a",
   "metadata": {
    "id": "ORVJTRAh-Z2a"
   },
   "source": [
    "Available at http://www.comp.nus.edu.sg/~cs3244/AY2122S1/01.assignment.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbae3e4-d223-47a6-9825-f548841c5a57",
   "metadata": {
    "id": "fbbae3e4-d223-47a6-9825-f548841c5a57"
   },
   "source": [
    "![Machine Learning](http://www.comp.nus.edu.sg/~cs3244/AY2122S1/2110-header.png)\n",
    "---\n",
    "<!-- See **Credits** below for acknowledgements and rights.  For NUS class credit, you'll need to do the corresponding _Assessment_ in [CS3244 in Coursemology](http://coursemology.org/courses/1870) by the respective deadline (mentioned at the end of the notebook).  -->\n",
    "\n",
    "**You must acknowledge that your submitted assignment in LumiNUS is your independent work and that you followed course policy.  This assignment is due on 5 Sep 2021 (Sun) at 23:59 pm (i.e., before Monday).  Make sure you reserve sufficient time to fill in the corresponding LumiNUS Quiz and uploading your exported copy to LumiNUS Files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TZIaecT9uF6r",
   "metadata": {
    "id": "TZIaecT9uF6r"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "We have introduced $k$**-Nearest Neighbors** and **Decision Trees** in the lectures on Weeks 02 and 03. In this **individual** assignment, you will be graded on implementing the models yourself. You will run both models and compare their performance on the same dataset. Overall, you will learn to appreciate the similarities and differences in models to predict on the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hFe14MROoltp",
   "metadata": {
    "id": "hFe14MROoltp"
   },
   "source": [
    "## Assignment Instructions \n",
    "\n",
    "Before the assignment, you should create a copy of this Colab file in your own Google Drive. \n",
    "\n",
    "In this assignment, you will\n",
    "\n",
    "1. Follow the instructions to familiarize yourself with the `wine` dataset;\n",
    "2. Write your code in the designated spaces to finish the implementations of the $k$-Nearest Neighbor and Decision Tree algorithms;\n",
    "2. Run the notebook to obtain your results;\n",
    "3. Copy each of your code and result, and  paste into the Luminus quiz submission.\n",
    "4. Upload this Colab file into Luminus \"Files/Assignment#1 Submissions\n",
    "\n",
    "**Assignment Deadline: September 5th (Sunday) 23:59 SGT**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8",
   "metadata": {
    "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8"
   },
   "source": [
    "## 1. Programming: $k$-NN and Decision Tree from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0",
   "metadata": {
    "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0"
   },
   "source": [
    "### .a Loading and Visualizing Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88",
   "metadata": {
    "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88"
   },
   "source": [
    "\n",
    "We'll use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset from the popular [UCI dataset repository](https://archive.ics.uci.edu/ml/index.php).  This describes the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 chemical constituents found in each of the three types of wines.\n",
    "\n",
    "We'll load in the data for white wines, take a look around and split the data into parts for training the model and testing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebed0d46-50bd-447d-9292-256bfacab8cd",
   "metadata": {
    "id": "ebed0d46-50bd-447d-9292-256bfacab8cd"
   },
   "outputs": [],
   "source": [
    "# Import the standard tools for pythonic data analysis.\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# Let's read the data in as a \"data frame\" (df), equivalent to our D = (X,y) data matrix\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',sep=';') # Separate on semicolons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb",
   "metadata": {
    "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb"
   },
   "source": [
    "We want to apply matchine learning algorithms to predict the quality of the wine from its constituents. That is, we will utilize algorithms that can find the correlation between wine quality and its 13 constituent (as features) to make prediction. To better understand how algorithms can achieve this, we need to inspect the data distribution first.\n",
    "<!-- (actually, we can predict any feature from any other feature, there's really no particular distinction between $\\mathbf{x}$ and $y$).   -->\n",
    "Let's take a look at the distribution for **quality**. This tells us the how the wine quality is distributed without knowing anything about other information. Go ahead and run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
    "outputId": "09038aa1-9d64-4746-e29d-942c34b44e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wines of a particular rating:\n",
      "6    2198\n",
      "5    1457\n",
      "7     880\n",
      "8     175\n",
      "4     163\n",
      "3      20\n",
      "9       5\n",
      "Name: quality, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'quality'}>]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get numeric distributions of our quality output\n",
    "print(\"Number of wines of a particular rating:\")\n",
    "counts = df['quality'].value_counts()\n",
    "print(counts)\n",
    "\n",
    "# Let's do a histogram plot. To do that we need to specify the \n",
    "# y-axis that we want to plot – i.e. 'quality' – and number of bins\n",
    "df.hist(column = 'quality', bins = len(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1",
   "metadata": {
    "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1"
   },
   "source": [
    "This looks somewhat normally distributed.  Let's say we care about good wines.  Then we'll just concentrate on differentiating great wines from the rest. Our task is now a classification. As we can see from the histogram below, the two classes are heavily imbalanced -- we have far more \"Not Good\" wines than the \"Good\" wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
    "outputId": "756d05c4-305e-4180-fb90-e8a2e1202d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of classes'}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new column called good wine, where the value of the quality is 7 or better.\n",
    "df['good wine'] = np.where(df['quality']>=7, \"Good\", \"Not Good\")\n",
    "\n",
    "# Then remove the quality column (why)\n",
    "df.drop('quality', axis=1, inplace=True)\n",
    "\n",
    "df['good wine'].value_counts().plot(kind = 'bar', title = 'Distribution of classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34552f-c406-440a-a3fa-86e9337e7013",
   "metadata": {
    "id": "2e34552f-c406-440a-a3fa-86e9337e7013"
   },
   "source": [
    "### .b Split dataset into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a",
   "metadata": {
    "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a"
   },
   "source": [
    "Let's split the data into training and testing data sets before we do anything else.   It's important to look only at the training data to develop our intuitions (**why?**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
    "outputId": "c68224cb-1055-4035-9c56-9c2c335defde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances:  3918 \n",
      "Number of test instances:  980\n"
     ]
    }
   ],
   "source": [
    "# Learn how to split test and training data from a whole\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Partion the features from the class to predict\n",
    "df_X = df[df.columns[df.columns != 'good wine']].copy() # get columns that are not 'good wine'\n",
    "df_y = df['good wine'].copy() # get the column named 'good wine'; this is our label\n",
    "\n",
    "# (random_state): we use a fixed random seed so we get the same results every time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=0)\n",
    "\n",
    "print (\"Number of training instances: \", len(X_train), \"\\nNumber of test instances: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b988fbc-d3ec-492b-b314-46a8306d2040",
   "metadata": {
    "id": "0b988fbc-d3ec-492b-b314-46a8306d2040"
   },
   "source": [
    "Let's look at the first few rows of the training portion of the dataset.  Understanding the data is **always** important in trying to build any model for prediction.\n",
    "You can check against the description of the dataset which is here: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
    "outputId": "0e220a49-26cb-4a77-99bc-fc74c99d6044"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4685</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>10.4</td>\n",
       "      <td>0.062</td>\n",
       "      <td>23.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.99660</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.49</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.029</td>\n",
       "      <td>31.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.98895</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.36</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.99165</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.054</td>\n",
       "      <td>51.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0.99820</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>6.1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>12.6</td>\n",
       "      <td>0.064</td>\n",
       "      <td>63.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.99940</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0.43</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4685            7.2              0.26         0.32            10.4      0.062   \n",
       "4589            5.4              0.29         0.38             1.2      0.029   \n",
       "2700            6.5              0.46         0.31             5.0      0.027   \n",
       "283             6.7              0.34         0.30            15.6      0.054   \n",
       "1014            6.1              0.16         0.27            12.6      0.064   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4685                 23.0                 114.0  0.99660  3.23       0.49   \n",
       "4589                 31.0                 132.0  0.98895  3.28       0.36   \n",
       "2700                 15.0                  72.0  0.99165  3.26       0.60   \n",
       "283                  51.0                 196.0  0.99820  3.19       0.49   \n",
       "1014                 63.0                 162.0  0.99940  3.66       0.43   \n",
       "\n",
       "      alcohol  \n",
       "4685     10.5  \n",
       "4589     12.4  \n",
       "2700     11.5  \n",
       "283       9.3  \n",
       "1014      8.9  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485",
   "metadata": {
    "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485"
   },
   "source": [
    "We can also take a look at some general statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
    "outputId": "0d2ea328-bf7c-45b7-eb98-fee6818c99cb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "      <td>3918.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.853714</td>\n",
       "      <td>0.278401</td>\n",
       "      <td>0.332726</td>\n",
       "      <td>6.366807</td>\n",
       "      <td>0.045862</td>\n",
       "      <td>35.352348</td>\n",
       "      <td>138.448698</td>\n",
       "      <td>0.994005</td>\n",
       "      <td>3.187981</td>\n",
       "      <td>0.490439</td>\n",
       "      <td>10.518579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.825378</td>\n",
       "      <td>0.100316</td>\n",
       "      <td>0.119874</td>\n",
       "      <td>4.994107</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>16.885968</td>\n",
       "      <td>42.228313</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.150777</td>\n",
       "      <td>0.115614</td>\n",
       "      <td>1.233287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>0.991700</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993715</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.010300</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    3918.000000       3918.000000  3918.000000     3918.000000   \n",
       "mean        6.853714          0.278401     0.332726        6.366807   \n",
       "std         0.825378          0.100316     0.119874        4.994107   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.315000        5.200000   \n",
       "75%         7.300000          0.320000     0.380000        9.800000   \n",
       "max        11.800000          1.100000     1.660000       31.600000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  3918.000000          3918.000000           3918.000000  3918.000000   \n",
       "mean      0.045862            35.352348            138.448698     0.994005   \n",
       "std       0.022008            16.885968             42.228313     0.002932   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            109.000000     0.991700   \n",
       "50%       0.043000            34.000000            134.000000     0.993715   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.010300   \n",
       "\n",
       "                pH    sulphates      alcohol  \n",
       "count  3918.000000  3918.000000  3918.000000  \n",
       "mean      3.187981     0.490439    10.518579  \n",
       "std       0.150777     0.115614     1.233287  \n",
       "min       2.740000     0.220000     8.000000  \n",
       "25%       3.090000     0.410000     9.500000  \n",
       "50%       3.180000     0.470000    10.400000  \n",
       "75%       3.280000     0.550000    11.400000  \n",
       "max       3.810000     1.080000    14.200000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58",
   "metadata": {
    "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58"
   },
   "source": [
    "### .c Train and Test the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06",
   "metadata": {
    "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06"
   },
   "source": [
    "Actually all of the above was just preparation. Now we have some intution, ;let's try fitting the dataset on both $k$ - NN and Decision Tree models using sklearn's API library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
    "outputId": "9fea22c2-2030-48d3-aa4b-fc5638d3d817"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN accuracy for training set: 1.000000\n",
      "kNN accuracy for test set: 0.802041\n"
     ]
    }
   ],
   "source": [
    "# Get the machine learning algorithm k-NN\n",
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1, metric='euclidean')\n",
    "knn_model = knn.fit(X_train, y_train)\n",
    "print('kNN accuracy for training set: %f' % knn_model.score(X_train, y_train))\n",
    "print('kNN accuracy for test set: %f' % knn_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
    "outputId": "fb3c9d08-a693-4658-8611-3c70d284de2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy for training set: 1.000000\n",
      "Decision Tree accuracy for test set: 0.813265\n"
     ]
    }
   ],
   "source": [
    "# Get the machine learning algorithm Naïve Bayes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=None)\n",
    "dt_model = dt.fit(X_train,y_train)\n",
    "print('Decision Tree accuracy for training set: %f' % dt_model.score(X_train, y_train))\n",
    "print('Decision Tree accuracy for test set: %f' % dt_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e",
   "metadata": {
    "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e"
   },
   "source": [
    "After running these experiments you should have been able to get about 80% accuracy (on test set) for the nearest neighbor code and about 81% accuracy (on test set) using the Decision Tree algorithm.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CDmjJP10R4Kd",
   "metadata": {
    "id": "CDmjJP10R4Kd"
   },
   "source": [
    "That was easy! But it is important to know how to master these models yourself, so you're going to implement from scratch.  You'll practice this in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2",
   "metadata": {
    "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2"
   },
   "source": [
    "To ease the implementation in the next section, we'll first transform the data into `numpy` arrays and transform the labels into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f",
   "metadata": {
    "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f75dd60dce2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
    "y_train, y_test = y_train.to_list(), y_test.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f64225-4087-4081-ac7d-641b4718be7d",
   "metadata": {
    "id": "c7f64225-4087-4081-ac7d-641b4718be7d"
   },
   "source": [
    "## 2. Programming : Implement $k$-Nearest Neighbor (kNN)\n",
    "\n",
    "We are going to implement a kNN to predict whether the wine is \"Good\" or \"Not good\" (so it is a binary or 2-class classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823",
   "metadata": {
    "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823"
   },
   "source": [
    "Next we are going to build a list of helper functions to help build the kNN model. Your task is to implement these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94665f-50da-4bad-9544-36845fbc8aa3",
   "metadata": {
    "id": "3b94665f-50da-4bad-9544-36845fbc8aa3"
   },
   "source": [
    "### .a Calculate Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221",
   "metadata": {
    "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221"
   },
   "source": [
    "**Euclidean Distance**: Euclidean Distance measures the length of the line segment bewteen two points in the Euclidean space.  This is also referred to as L2 distance or L2 vector norm.\n",
    "\n",
    "$$\n",
    "L2(a, b) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\cdots + (a_n-b_n)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1",
   "metadata": {
    "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1"
   },
   "source": [
    "**Your Turn (Question 1):** Complete the code below to calculate the Euclidean Distance between two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176",
    "outputId": "fd26c72b-35e8-41fc-dd1d-1163c04073ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.92154581774061"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      a (D) : a data point in numpy array of dimension D\n",
    "      b (D) : a data point in numpy array of dimension D\n",
    "    Returns:\n",
    "      dis(float): the Euclidean distance between the two data points\n",
    "    \"\"\"\n",
    "    dis = 0\n",
    "    size = a.size\n",
    "    ###########################\n",
    "    #\n",
    "    # Your Turn (Q1): Write your code here\n",
    "    # Hint: calculate vector c which is the element-wise difference between vector a and b; find out the euclidean norm of c\n",
    "    # Hint: use the numpy library to speed up matrix and vector operations like difference, summation and square root. https://numpy.org/doc/stable/user/quickstart.html\n",
    "    #\n",
    "\n",
    "    # dist = sqrt((xa-xb)^2 + (ya-yb)^2 + (za-zb)^2)\n",
    "\n",
    "    for index in range(size):\n",
    "        dis = dis + math.pow(np.take(a, index) - np.take(b, index), 2)\n",
    "    return math.sqrt(dis)\n",
    "    ###########################\n",
    "# euclidean_distance(np.array([1,2,3]), np.array([3,4,5]))\n",
    "# euclidean_distance(X_test[12], X_train[12])\n",
    "euclidean_distance(X_test[25], X_train[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ts9nXApCGMzi",
   "metadata": {
    "id": "ts9nXApCGMzi"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87cb63e-dd55-4153-8a64-e8735a993be3",
   "metadata": {
    "id": "e87cb63e-dd55-4153-8a64-e8735a993be3"
   },
   "source": [
    "### .b Calculate Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b36013-a3f1-4bcf-af8f-979b753fe09e",
   "metadata": {
    "id": "60b36013-a3f1-4bcf-af8f-979b753fe09e"
   },
   "source": [
    "**Manhattan Distance**: Manhattan Distance measures the sum of the absolute differences of two vectors' Cartesian coordinates.\n",
    "\n",
    "$$\n",
    "L1(a, b) = |a_1-b_1| + |a_2-b_2| + \\cdots + |a_n-b_n|\n",
    "$$\n",
    "\n",
    "It's also referred to as the L1 distance or L1 vector norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06fdf2-befa-4adb-8a09-f411fa521c48",
   "metadata": {
    "id": "ee06fdf2-befa-4adb-8a09-f411fa521c48"
   },
   "source": [
    "**Your Turn (Question 2):** Complete the code below to calculate the Euclidean Distance between two data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c77a0441-8b6e-4f46-85f4-28d1ac2adb4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c77a0441-8b6e-4f46-85f4-28d1ac2adb4b",
    "outputId": "a95837aa-a2f1-4dd2-f705-38305f6813bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.52438"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manhattan_distance(a, b):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      a (D) : a data point in numpy array of dimension D\n",
    "      b (D) : a data point in numpy array of dimension D\n",
    "    Returns:\n",
    "      dis(float): the Manhattan distance between the two data points\n",
    "    \"\"\"\n",
    "    dis = 0\n",
    "    size = a.size\n",
    "    ###########################\n",
    "    #\n",
    "    # Your Turn (Q2): Write your code here\n",
    "    # Hint: calculate vector c which is the element-wise absolute difference between vector a and b; find out the sum of all elements in c\n",
    "    # Hint: use the numpy library to speed up matrix and vector operations like difference, summation and square root. https://numpy.org/doc/stable/user/quickstart.html\n",
    "    #\n",
    "    for index in range(size):\n",
    "        dis = dis + np.absolute(np.take(a, index) - np.take(b, index))\n",
    "    return dis\n",
    "    ###########################\n",
    "   \n",
    "\n",
    "# manhattan_distance(np.array([1,2,3]), np.array([3,4,5]))\n",
    "# manhattan_distance(X_test[17], X_train[17])\n",
    "manhattan_distance(X_test[80], X_train[80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HKrVZUZEAcTx",
   "metadata": {
    "id": "HKrVZUZEAcTx"
   },
   "source": [
    "**Testing**: test to see if the distance is consistent with your manual calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9u-RXoP2Bx8v",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9u-RXoP2Bx8v",
    "outputId": "b10c6c66-4a7e-475f-ea6f-3e7de4bcbf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance between (1, 2, 3) and (0, 0, 0): 3.7416573867739413\n",
      "Manhattan Distance between (1, 2, 3) and (0, 0, 0): 6\n",
      "Euclidean Distance between (100, 20, 30) and (0, 0, 0): 106.30145812734649\n",
      "Manhattan Distance between (100, 20, 30) and (0, 0, 0): 150\n"
     ]
    }
   ],
   "source": [
    "x1 = np.asarray([1, 2, 3])\n",
    "x2 = np.asarray([0, 0, 0])\n",
    "print('Euclidean Distance between (1, 2, 3) and (0, 0, 0):', euclidean_distance(x1, x2))\n",
    "print('Manhattan Distance between (1, 2, 3) and (0, 0, 0):', manhattan_distance(x1, x2))\n",
    "\n",
    "x1 = np.asarray([100, 20, 30])\n",
    "x2 = np.asarray([0, 0, 0])\n",
    "print('Euclidean Distance between (100, 20, 30) and (0, 0, 0):', euclidean_distance(x1, x2))\n",
    "print('Manhattan Distance between (100, 20, 30) and (0, 0, 0):', manhattan_distance(x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sMP2yG7DT8Gn",
   "metadata": {
    "id": "sMP2yG7DT8Gn"
   },
   "source": [
    "From the tests above, we can see that the Euclidean Distance is closer to the dimension in which the two vectors have the largest difference, *e.g.*, $(100-0)$, $(3-0)$. **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "CmfD_TS4A6a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmfD_TS4A6a3",
    "outputId": "f6830855-e3ba-450b-8e1c-5bdfd2f64a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.02941150258765\n",
      "73.48616\n"
     ]
    }
   ],
   "source": [
    "# Testing: This should return 52.02941150258766\n",
    "print(euclidean_distance(X_test[0], X_train[0]))\n",
    "\n",
    "# Testing: This should return 73.48616\n",
    "print(manhattan_distance(X_test[0], X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7",
   "metadata": {
    "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7"
   },
   "source": [
    "### .b Find k Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eea814-4b5c-4437-bd63-27b508fcc89e",
   "metadata": {
    "id": "13eea814-4b5c-4437-bd63-27b508fcc89e"
   },
   "source": [
    "**$k$-NN function**: The $k$-NN function will find the k nearest data points given an array of distances. We want to return the corresponding labels of the k Nearest Neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c",
   "metadata": {
    "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c"
   },
   "source": [
    "**Your Turn (Question 3):** Complete the code below to find out the kNN's labels with the given array of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "603a33b7-431f-4b37-b685-08e5556e578b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "603a33b7-431f-4b37-b685-08e5556e578b",
    "outputId": "d5d8d319-e500-4eed-ed6f-87c2eed3eed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def find_kNN_labels(distances, labels, k):\n",
    "    \n",
    "    knn_labels = []\n",
    "\n",
    "    k_nearest_neighbors = np.argsort(distances)[:k]\n",
    "    for x in k_nearest_neighbors:\n",
    "      knn_labels.append(np.take(labels, x));\n",
    "\n",
    "    return knn_labels\n",
    "\n",
    "# Testing: This should return [2, 1, 0, 1, 0]\n",
    "sample_distances = [1.97, 1.94, 0.16, 0.91, 2.05, 1.5, 1.86, 2.01, 1.9, 1.67, 1.9, 1.14, 2.1, 1.94, 2.68, 0.08, 3.98, 3.05, 1.59, 2.4]\n",
    "sample_labels = [0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 2, 1]\n",
    "find_kNN_labels(sample_distances, sample_labels, 5)\n",
    "\n",
    "# Test 1\n",
    "np.random.seed(0)\n",
    "\n",
    "sample_distances = np.random.randn(20)\n",
    "\n",
    "sample_labels = np.random.randint(low=0, high=10, size=20)\n",
    "\n",
    "labels = find_kNN_labels(sample_distances, sample_labels, 1)\n",
    "\n",
    "print(labels[0])\n",
    "\n",
    "#Test 2\n",
    "np.random.seed(2)\n",
    "\n",
    "sample_distances = np.random.randn(20)\n",
    "\n",
    "sample_labels = np.random.randint(low=0, high=10, size=20)\n",
    "\n",
    "labels = find_kNN_labels(sample_distances, sample_labels, 1)\n",
    "\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2FbdUK94lZpp",
   "metadata": {
    "id": "2FbdUK94lZpp"
   },
   "source": [
    "### .c Find the Majority Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1P6SRD1Mmudn",
   "metadata": {
    "id": "1P6SRD1Mmudn"
   },
   "source": [
    "**Majority Class Function**: When finding the $k$-Nearest Neighbors, we return the value that represents the majority of the $k$ instances of the class as the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cs6Px7tkl-ud",
   "metadata": {
    "id": "Cs6Px7tkl-ud"
   },
   "source": [
    "**Your Turn (Question 4):** Complete the code below to find the majority class from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ri7bAZaQlXTV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ri7bAZaQlXTV",
    "outputId": "89a76183-5757-43ab-e72f-7d1b5521f46b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_majority_class(labels):\n",
    "   \n",
    "    major = \"\"\n",
    "    \n",
    "    # freq will store the number of occurences of the target labels  -> Dictionary\n",
    "    freq = {}\n",
    "    for entry in labels:\n",
    "        if (entry in freq):\n",
    "            freq[entry] += 1.0\n",
    "        else:\n",
    "            freq[entry] = 1.0\n",
    "            \n",
    "    major = \"\"\n",
    "    sorted(freq)\n",
    "    major = sorted(freq, reverse= True)[0]\n",
    "\n",
    "    return major  \n",
    "  \n",
    "\n",
    "# Testing: This should return 'Not Good'\n",
    "sample_y = y_train[:10]\n",
    "# get_majority_class(sample_y)\n",
    "# get_majority_class(y_train[10:12])\n",
    "get_majority_class(y_train[33:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e998f-51cf-43a9-a33a-bec29c01913a",
   "metadata": {
    "id": "366e998f-51cf-43a9-a33a-bec29c01913a"
   },
   "source": [
    "### .c Run $k$-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910",
   "metadata": {
    "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910"
   },
   "source": [
    "The following code `run_knn` is provided to you to run your kNN helper functions. \n",
    "\n",
    "You do not have to understand its code for the purpose of this exercise. Just run it and check the accuracy. \n",
    "\n",
    "Note that because the prediction of a single data points requires traversing the whole training set. The code is a bit slow. It can take $2\\sim 4$ minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff",
   "metadata": {
    "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff"
   },
   "outputs": [],
   "source": [
    "def run_knn(distance_metric_function):\n",
    "    k = 1\n",
    "    correct = 0\n",
    "    for test_entry, label in zip(X_test, y_test):\n",
    "\n",
    "        ## find out the distance between the test data point and all training data points\n",
    "        distances = []\n",
    "        for train_entry in X_train:\n",
    "            distances.append(distance_metric_function(test_entry, train_entry))\n",
    "\n",
    "        knn_labels = find_kNN_labels(distances, y_train, k)\n",
    "        prediction = get_majority_class(knn_labels)\n",
    "        \n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(y_test)\n",
    "    print('Final accuracy')\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f",
   "metadata": {
    "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f"
   },
   "source": [
    "The following code runs $k$-NN with the Euclidean distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
    "outputId": "138aa8c8-6377-4410-c1b0-ad47f1ef5a6f"
   },
   "outputs": [],
   "source": [
    "## The accuracy should be 0.8020408163265306.\n",
    "run_knn(euclidean_distance) # note that euclidean_distance is a function, not a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac",
   "metadata": {
    "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac"
   },
   "source": [
    "The following code runs $k$-NN with the Manhattan distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f4d42-9a0c-448b-8440-b8c3c8927853",
   "metadata": {
    "id": "858f4d42-9a0c-448b-8440-b8c3c8927853"
   },
   "outputs": [],
   "source": [
    "## The accuracy should be 0.8122448979591836\n",
    "run_knn(manhattan_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qv_62H2BGR2t",
   "metadata": {
    "id": "Qv_62H2BGR2t"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5",
   "metadata": {
    "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5"
   },
   "source": [
    "## 3. Programming : Implement Decision Tree\n",
    "\n",
    "We are going to implement a decision tree to predict whether the wine is \"Good\" or \"Not good\" (again, it is a 2-class classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3",
   "metadata": {
    "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3"
   },
   "source": [
    "### .a Discretize the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8",
   "metadata": {
    "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8"
   },
   "source": [
    "We are going to make use of 11 _attributes_ (also known as _features_ or _input dimensions_, but for this decision trees to be consistent with the algorithm, we'll use \"attributes\"). \n",
    "\n",
    "In this assignment, we will implement a basic version of Decision tree that takes in categorical attributes. Thus, we are going to discretize the continuous features, where values **larger** than the mean is assigned **1** and values **smaller** than the mean is assigned **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e",
   "metadata": {
    "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e"
   },
   "outputs": [],
   "source": [
    "def discretize_data(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_data (np array): the training set\n",
    "        test_data (np array): the test set\n",
    "    \n",
    "    Returns:\n",
    "    np_train_data (np array): contains the discretized training set\n",
    "    np_test_data (np array): contains the discretized test set\n",
    "    \"\"\"\n",
    "    train_data_discrete = np.zeros_like(train_data) # initialize\n",
    "    test_data_discrete = np.zeros_like(test_data)\n",
    "    \n",
    "    D = train_data_discrete.shape[1]\n",
    "    \n",
    "    for i in range(D):\n",
    "        mean_value = np.mean(train_data[:, i]) # get mean values at which to label as 1 (larger) or 0 (smaller)\n",
    "        train_data_discrete[:, i] = (train_data[:, i] > mean_value).astype(float)\n",
    "        test_data_discrete[:, i] = (test_data[:, i] > mean_value).astype(float)\n",
    "    \n",
    "    return train_data_discrete, test_data_discrete\n",
    "\n",
    "X_train_discrete, X_test_discrete = discretize_data(X_train, X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06203550-afe2-4496-b0b8-53da94f6b5c1",
   "metadata": {
    "id": "06203550-afe2-4496-b0b8-53da94f6b5c1"
   },
   "outputs": [],
   "source": [
    "sample_X = X_train_discrete[:10]\n",
    "sample_y = y_train[:10]\n",
    "sample_X, sample_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea",
   "metadata": {
    "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea"
   },
   "source": [
    "### .b Choose the best Feature based on Majority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1",
   "metadata": {
    "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1"
   },
   "source": [
    "**Majority Class Function**: When you reach a terminal node (leaf) in a decision tree, we return the label that represents the majority of the sub-dataset's instances of the class as the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bv2g3fB_mCQS",
   "metadata": {
    "id": "Bv2g3fB_mCQS"
   },
   "source": [
    "We can reuse the `get_majority_class` function from the implementation of $k$-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed",
   "metadata": {
    "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed"
   },
   "source": [
    "### .c Calculate Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f",
   "metadata": {
    "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f"
   },
   "source": [
    "**Entropy Function**: Calculate the entropy of this current sub-dataset:\n",
    "\n",
    "Recall: $H(X) = - \\sum_{i=0}^C p_i\\log(p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b",
   "metadata": {
    "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b"
   },
   "source": [
    "**Your Turn (Question 5):** Complete the code below to calculate entropy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426b56e-ab12-416f-b9b0-dd91099d828e",
   "metadata": {
    "id": "3426b56e-ab12-416f-b9b0-dd91099d828e"
   },
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
    "          m: num_rows\n",
    "    Returns:\n",
    "      dataEntropy: The entropy of this current sub-dataset\n",
    "    \"\"\"\n",
    "    dataEntropy = 0.0\n",
    "    # freq will store the number of occurrences of the target labels\n",
    "    freq = {}\n",
    "    for entry in labels:\n",
    "        if (entry in freq):\n",
    "            freq[entry] += 1.0\n",
    "        else:\n",
    "            freq[entry] = 1.0\n",
    "\n",
    "    ###########################\n",
    "    #\n",
    "    # Your Turn (Q5): Write your code here\n",
    "    # Hint: Loop through each row of data, get the last column, find number of occurrences of each value, then use the above equation.\n",
    "    #\n",
    "    # freq_values = np.array(freq.values()[0])\n",
    "    for x in freq.values():\n",
    "      p = x / len(labels)\n",
    "      dataEntropy = dataEntropy + (-(p*math.log2(p)))\n",
    "  \n",
    "    ###########################\n",
    "    \n",
    "    return dataEntropy\n",
    "\n",
    "# Testing: This should return 0.4689955935892812 (in log 2)\n",
    "\n",
    "entropy(y_train[30:60]) \n",
    "entropy(y_train[100:120])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2",
   "metadata": {
    "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2"
   },
   "source": [
    "### .d Calculate Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b",
   "metadata": {
    "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b"
   },
   "source": [
    "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
    "\n",
    "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
    "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
    "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
    "\n",
    "<div align=\"center\">\n",
    "$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$\n",
    "</div>\n",
    "\n",
    "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
    "<div align=\"center\">\n",
    "$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $  \n",
    "</div>\n",
    "\n",
    "Subsequently, we choose the attribute with the largest IG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541937a-d959-4e6d-84bc-9610b83daa66",
   "metadata": {
    "id": "8541937a-d959-4e6d-84bc-9610b83daa66"
   },
   "source": [
    "**Your Turn (Question 6):** Complete the code below to calculate information gain of a given attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08aa3c-16fa-498f-8b16-57d571126ad3",
   "metadata": {
    "id": "de08aa3c-16fa-498f-8b16-57d571126ad3"
   },
   "outputs": [],
   "source": [
    "def info_gain(data, labels, attribute, attributes):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      data(m, D): The current sub-dataset of the sub-tree\n",
    "          m: num_rows\n",
    "          D: num_features\n",
    "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
    "          m: num_rows\n",
    "      attribute: The attribute used to split data\n",
    "      attributes: The list of current remaining attributes\n",
    "    Returns:\n",
    "      info_gain : information gain of the given dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    freq = {}\n",
    "    subsetEntropy = 0.0\n",
    "    \n",
    "    # Get the column index of this attribute\n",
    "    i = attributes.index(attribute)\n",
    "    \n",
    "    for entry in data:\n",
    "        if (entry[i] in freq):\n",
    "            freq[entry[i]] += 1.0\n",
    "        else:\n",
    "            freq[entry[i]]  = 1.0\n",
    "\n",
    "    ###########################\n",
    "    #\n",
    "    # Your Turn (Q6): Write your code here\n",
    "    # Hint: Split the data based on the value at index i. Find the subsetEntropy of\n",
    "    # each sub-dataset, then use the formula of Information Gain to calculate subsetEntropy.\n",
    "    #\n",
    "    print(data)\n",
    "    print(labels)\n",
    "    print(freq)\n",
    "    print(freq.keys())\n",
    "    print(attributes)\n",
    "    for x in list(freq.keys()) :\n",
    "      probality = freq[x] / sum(freq.values())\n",
    "      dataSub    = [entry for entry in data if entry[i] == x]\n",
    "      subsetEntropy = probality * entropy(dataSub)\n",
    "      \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    return (entropy(labels) - subsetEntropy)\n",
    "\n",
    "# Testing: This should return 0.054824648581652036\n",
    "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "info_gain(sample_X, sample_y, 'residual sugar', attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b",
   "metadata": {
    "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b"
   },
   "source": [
    "### .e Find the Best Attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418",
   "metadata": {
    "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418"
   },
   "source": [
    "Now we will write a function to choose the **best** (most discriminating) attribute for a given data, here best indicates having **largest** information gain (IG) among all attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b03b21-a83c-4b15-9eec-261c552af288",
   "metadata": {
    "id": "f9b03b21-a83c-4b15-9eec-261c552af288"
   },
   "source": [
    "**Your Turn (Question 7):** Complete the code below to get the best attribute based on information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a",
   "metadata": {
    "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a"
   },
   "outputs": [],
   "source": [
    "def get_best_gain_attribute(data, labels, attributes):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "      data(m, D): The current sub-dataset of the sub-tree\n",
    "          m: num_rows\n",
    "          D: num_features + 1 (last column is the label)\n",
    "      attributes: The list of current remaining attributes\n",
    "    Returns:\n",
    "      best: The best attribute to split based on info gain.\n",
    "    \"\"\"\n",
    "    best = attributes[0]\n",
    "    \n",
    "    max_gain = 0\n",
    "    for attr in attributes:\n",
    "      ###########################\n",
    "      #\n",
    "      # Your Turn (Q7): Write your code here\n",
    "      # Hint: For each attribute in attributes, use info_gain function above\n",
    "      # to know which attribute is the best option\n",
    "      #\n",
    "      pass\n",
    "      ###########################\n",
    "    return best\n",
    "  \n",
    "# Testing: This should return 'chlorides'\n",
    "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "get_best_gain_attribute(sample_X, sample_y, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aab711-569d-4ace-849c-0684a1e420f7",
   "metadata": {
    "id": "b6aab711-569d-4ace-849c-0684a1e420f7"
   },
   "source": [
    "### .f Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196",
   "metadata": {
    "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196"
   },
   "source": [
    "We will define two helper functions here. First, `get_unique_values` returns the unique values of a given attribute. The second function, `get_sub_data` returns the subset of rows (instances) containing a specific value of a chosen attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5",
   "metadata": {
    "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5"
   },
   "outputs": [],
   "source": [
    "# These two functions are helper functions\n",
    "# This function will get unique values for that particular attribute from the given data\n",
    "def get_unique_values(data, attributes, attribute):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data (m,D) : Current subset of data\n",
    "      attributes : The list of current remaining attributes\n",
    "      attribute : Our point of interest\n",
    "    \n",
    "    Returns:\n",
    "      values : List of unique values for our point of interest\n",
    "    \"\"\"\n",
    "    index = attributes.index(attribute)\n",
    "    values = []\n",
    "    # \n",
    "    for entry in data:\n",
    "        if entry[index] not in values:\n",
    "            values.append(entry[index])\n",
    "\n",
    "    return values\n",
    "\n",
    "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
    "def get_sub_data(data, labels, attributes, best, val):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      data (m,D) : Current subset of data\n",
    "      labels (m) : Corresponding labels of current subset of data\n",
    "      attributes : The list of current remaining attributes\n",
    "      best : The attribute of which data we will extract\n",
    "      val : We are interested only on this value of the `best` attribute\n",
    "    Returns:\n",
    "      new_data : Data subset containing only those rows where `best` attribute = val\n",
    "      new_labels : Label subset containing only those values where `best` attribute = val\n",
    "    \"\"\"\n",
    "    new_data = [[]] \n",
    "    new_labels = []\n",
    "    attribute_index = attributes.index(best)\n",
    "\n",
    "    for index, entry in enumerate(data):\n",
    "        if (entry[attribute_index] == val):\n",
    "            newEntry = []\n",
    "            for i in range(0,len(entry)):\n",
    "                if(i != attribute_index):\n",
    "                    newEntry.append(entry[i])\n",
    "            new_data.append(newEntry)\n",
    "            new_labels.append(labels[index])\n",
    "    new_data.remove([])    \n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00",
   "metadata": {
    "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00"
   },
   "source": [
    "### .g Build the Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716217b-4105-4fb4-ab5f-40666d1d671e",
   "metadata": {
    "id": "a716217b-4105-4fb4-ab5f-40666d1d671e"
   },
   "source": [
    "In the below code, your task is to build a tree recursively. Starting at the root, pick the best attribute to split on, and call the `build_tree` function on each of the sub-trees.  Check the inlined code comments for clarification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a",
   "metadata": {
    "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a"
   },
   "source": [
    "**Your Turn (Question 8):** Complete the code below to build the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973",
   "metadata": {
    "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973"
   },
   "outputs": [],
   "source": [
    "def build_tree(data, labels, attributes):\n",
    "    \"\"\" \n",
    "      Args:\n",
    "        data(m, D): The current sub-dataset of the sub-tree\n",
    "            m: num_rows\n",
    "            D: num_features + 1 (last column is the label)\n",
    "        attributes: The list of current remaining attributes\n",
    "      Returns:\n",
    "        tree: The constructed tree as object. For example if the root is gender,\n",
    "              then a tree of depth 2 is like \n",
    "              {'gender': {'male': sub_tree1, 'female': sub_tree2}}\n",
    "    \"\"\"\n",
    "    data = data[:]\n",
    "    tree = {}\n",
    "    ##################################################################\n",
    "    ## Your Turn (Q8): Finish the implementation of the below code ##\n",
    "    if (len(data) == 0) or (len(attributes) - 1) <= 0: # What should we return if we run out of features to split?\n",
    "        return major_class(data)\n",
    "    elif vals.count(vals[0]) == len(vals): # What should we return if this sub-dataset is pure?\n",
    "        return vals[0]\n",
    "    else:\n",
    "        best = attr_choose(data, attributes)\n",
    "        tree = {best:{}}\n",
    "    \n",
    "        for val in get_unique_values(data, attributes, best):\n",
    "            # Calculate the subtree here\n",
    "            \n",
    "            new_data = get_data(data, attributes, best, val)\n",
    "            newAttr = attributes[:]\n",
    "            newAttr.remove(best)\n",
    "            subtree = build_tree(new_data, newAttr)\n",
    "            \n",
    "            tree[best][val] = subtree\n",
    "    ###########################\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2",
   "metadata": {
    "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2"
   },
   "source": [
    "The below code block containing `run_decision_tree` does exactly as its name, and you don't have to understand its code for the purpose of this exercise. Just run it and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a807fb-aebf-4b14-8d49-3b629878e847",
   "metadata": {
    "id": "16a807fb-aebf-4b14-8d49-3b629878e847"
   },
   "outputs": [],
   "source": [
    "# Class Node which will be used while classifying a test instance using the tree built (\"fit\") earlier\n",
    "class Node():\n",
    "    value = \"\"\n",
    "    children = []\n",
    "\n",
    "    def __init__(self, val, dictionary):\n",
    "        self.value = val\n",
    "        if (isinstance(dictionary, dict)):\n",
    "            self.children = list(dictionary.keys())\n",
    "\n",
    "def run_decision_tree():\n",
    "    attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
    "    tree = build_tree(X_train_discrete, y_train, attributes)\n",
    "    results = []\n",
    "\n",
    "    for entry, label in zip(X_test_discrete, y_test):\n",
    "        tempDict = tree.copy()\n",
    "        result = \"\"\n",
    "        while(isinstance(tempDict, dict)):\n",
    "            root = Node(list(tempDict.keys())[0], tempDict[list(tempDict.keys())[0]])\n",
    "            tempDict = tempDict[list(tempDict.keys())[0]]\n",
    "            index = attributes.index(root.value)\n",
    "            value = entry[index]\n",
    "            if(value in list(tempDict.keys())):\n",
    "                child = Node(value, tempDict[value])\n",
    "                result = tempDict[value]\n",
    "                tempDict = tempDict[value]\n",
    "            else:\n",
    "                result = \"Null\"\n",
    "                break\n",
    "        if result != \"Null\":\n",
    "            results.append(result == label)\n",
    "        \n",
    "    accuracy = float(results.count(True))/float(len(results))\n",
    "#     print(results)\n",
    "    print(\"FINAL ACCURACY: \")\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25076d80-85cc-47bb-b63f-c8956de81e4b",
   "metadata": {
    "id": "25076d80-85cc-47bb-b63f-c8956de81e4b"
   },
   "outputs": [],
   "source": [
    "run_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e574952-8958-4b4a-b1d9-65233d061840",
   "metadata": {
    "id": "4e574952-8958-4b4a-b1d9-65233d061840"
   },
   "source": [
    "## 4. Comparison between $k$-NN and Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FQ85usDdFUbI",
   "metadata": {
    "id": "FQ85usDdFUbI"
   },
   "source": [
    "The above is akin to a skeleton of a machine learning project.  Critical for understanding is knowing why the performance of a learner is as it is. As a scientist, you'll want to know why you think a learner performs well or poorly and **then** use experiments to verify your hunches.  To build up this skill we need to practice it, as you'll need to apply this in your own group projects later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b610577-3690-403c-b18f-93214458bda2",
   "metadata": {
    "id": "7b610577-3690-403c-b18f-93214458bda2"
   },
   "source": [
    "**Your Turn (Question 9):** Compare the performances of our implementation of $k$-NN and decision tree. Tell us what you have observed and why one of the models performs better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gBNkBYnOo3ks",
   "metadata": {
    "id": "gBNkBYnOo3ks"
   },
   "source": [
    "**Your answer here (Question 9)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VGlAWahG2MW9",
   "metadata": {
    "id": "VGlAWahG2MW9"
   },
   "source": [
    "**Your Turn (Question 10):** Compare the running times of $k$-NN and decision tree. Briefly tell us why $k$-NN is much slower than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gYlhuN-vpBTy",
   "metadata": {
    "id": "gYlhuN-vpBTy"
   },
   "source": [
    "**Your answer here (Question 10)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fSVen-P92HPP",
   "metadata": {
    "id": "fSVen-P92HPP"
   },
   "source": [
    "**Your Turn (Question 11):** Compare the performances of your implementation of decision tree and sklearn's implementation. Which one has the better performance? Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5W8zvwKapCvJ",
   "metadata": {
    "id": "5W8zvwKapCvJ"
   },
   "source": [
    "**Your answer here (Question 11)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h0ySjsCG3r9j",
   "metadata": {
    "id": "h0ySjsCG3r9j"
   },
   "source": [
    "**Your Turn (Question 12):** Identify an instance where $k$-NN predicts a different label than Decision Tree. Briefly tell us why you think they predicted differently, and what can you do to make them agree more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z3c_dfKQ3sAy",
   "metadata": {
    "id": "Z3c_dfKQ3sAy"
   },
   "source": [
    "**Your answer here (Question 12)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j5yziGEN4J1U",
   "metadata": {
    "id": "j5yziGEN4J1U"
   },
   "source": [
    "Congratulations 🎉 🎉 ! You have come to the end of the assignment. \n",
    "**Remember** to submit your \"Your Turn\" code, results, and answers in LumiNUS Quiz to be graded for your work."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_»_CS3244_Machine_Learning.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
